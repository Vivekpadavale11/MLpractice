{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMA8b61C3jDf"
      },
      "source": [
        "# Day 1: Text Pipeline - Your First Language Model\n",
        "\n",
        "Welcome to hands-on text processing! Now that you understand neural networks, let's explore how they work with text data.\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "- Understand how text becomes numbers (tokenization)\n",
        "- Load and use a pre-trained language model\n",
        "- Experiment with text generation parameters\n",
        "- Compare different prompt engineering techniques\n",
        "- Build your first text generation pipeline\n",
        "\n",
        "## üìö Research Focus\n",
        "This notebook emphasizes **discovery learning**. You'll:\n",
        "1. Research concepts before implementing\n",
        "2. Experiment with parameters to see their effects\n",
        "3. Compare different approaches\n",
        "4. Build understanding through hands-on exploration\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRw2ic4M3jDl"
      },
      "source": [
        "## 1. From Text to Numbers\n",
        "\n",
        "Neural networks work with numbers, but we have text. How do we bridge this gap?\n",
        "\n",
        "üîç **RESEARCH TASK 1**:\n",
        "- What is tokenization in NLP?\n",
        "- What is the difference between word-level and sub-word tokenization?\n",
        "- Research \"BPE\" (Byte Pair Encoding) - how does it work?\n",
        "- Why can't we just assign each word a number?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jGLzwiV3jDm",
        "outputId": "c819495b-0d16-42a7-b577-080e88c64dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1FE7XTq3jDq"
      },
      "source": [
        "### Exploring Tokenization\n",
        "\n",
        "üîç **RESEARCH TASK 2**:\n",
        "- Look up the GPT-2 tokenizer documentation\n",
        "- What is a \"vocabulary size\"?\n",
        "- What happens when the model encounters a word it's never seen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804,
          "referenced_widgets": [
            "5b5da33869214727b39f79a71e276723",
            "3e608464c09348189fce914094ac1621",
            "df1182a494674ba0a9ce9dfe5d2b9b61",
            "23c2de8e104c4973b8eba5ca6c6b7194",
            "35ca61001d9e4f46a5144f232ad995f4",
            "54665b8ae9d3426aad94dfcd730d31df",
            "d2729b171abc44ca95ef3c5e0bed3732",
            "cae1782d3fa340439b5073dc212b8d0e",
            "7991c8661486412aa3e1bff89df2b9c7",
            "649fbb3b1b0043989f3b22ef5b8d2b7d",
            "5a903d80cb794d2ba21bb4e09040f3e9",
            "546b690590f54f5780495ff04aa5d900",
            "87a63baad495475f8a6289c0a993f0b0",
            "410a09465a9443fdbf95c9ceb2c43f91",
            "57c08b2ee5cb4c9793a8abfaaba801ca",
            "3c3194c1ec714ea4beb9d3c3a19e1d2e",
            "0b0d163e814848a0bf976a01f9b59bcb",
            "a56dcb8a246149b6aa62ddc1feb994a6",
            "0417fa3d77da46db9a37e6e01260e0f8",
            "4a820609e4af47979ed0de7462870951",
            "c8d9b134587343ba860ff26e8e9f057e",
            "de81bb6f00434967b900836fde989614",
            "b09645f9251d4dfe955711691cb05d76",
            "6156c51fbb514a218430b586aa13886d",
            "f9fc77c9956c476ab797452a61517e45",
            "7ec8de49c49c42fb85f8249aa668852b",
            "7cab583f55434e58920fe19d5c63cea3",
            "183efac944d64bf591bd7aa0f7306a96",
            "1d07d96ccde940bd8ac9cc050e9a0f36",
            "5d620339c3064806901e127c47aed7d0",
            "db9e7f13eefb47b093ea3d2176fe2e29",
            "c2fd2436138f4e95a6f04fdf243bcd87",
            "2a826b1d535044469f74230264062df3",
            "25aebc31922e4c27915ee07b97f95538",
            "ce3f5178706d454dbe4ba9d632d0e266",
            "3f1164b9c54549d9a455df2652e70e3b",
            "82c13086eb064e1696f3dacfe618b7e8",
            "c3df7b243ed34fcf8cc5e168f440d525",
            "2257d5135cbe4c9f8d20c965183cbb77",
            "d36100c29d0b43e48322c4fb793bca7b",
            "352b4085842c44b09817d7b7fd860060",
            "9b2a5f59ae5f47d8a3271f61e4428fd4",
            "2c798853dc9b4c48add9d4b3078f9771",
            "b3522c2f92ec47e7bbfa674609c8d804",
            "21aaff715ba04aef966e013e01503d93",
            "7ca8aae92ed44e9796c8a8e9487f9783",
            "3fbacb21b0cf42fba7ae25f79e373868",
            "fc38d2a7531746a2b2cebb2e47278b06",
            "90ff404117be460db2c7508e658337bb",
            "7b03b82607ab4d56928602bfa8000d95",
            "15a79aefa99a4a5d91e16971272112e4",
            "b5c80d526c5640d9aa4c76423a6492d7",
            "4671cf38a0f8425e8febce2a876c3d16",
            "fab7a2bc93ae4e089dd8539c47ab2e76",
            "84f97943ab474a74a0dbe135e7e2e410"
          ]
        },
        "id": "OY_XHBP93jDr",
        "outputId": "ac2e8fa4-fa21-4f0c-c93a-ebcdbce11c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b5da33869214727b39f79a71e276723"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "546b690590f54f5780495ff04aa5d900"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b09645f9251d4dfe955711691cb05d76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25aebc31922e4c27915ee07b97f95538"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21aaff715ba04aef966e013e01503d93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Exploring Tokenization:\n",
            "==================================================\n",
            "\n",
            "Original: Hello world!\n",
            "Tokens: ['Hello', 'ƒ†world', '!']\n",
            "Token IDs: [15496, 995, 0]\n",
            "Number of tokens: 3\n",
            "\n",
            "Original: The quick brown fox jumps over the lazy dog.\n",
            "Tokens: ['The', 'ƒ†quick', 'ƒ†brown', 'ƒ†fox', 'ƒ†jumps', 'ƒ†over', 'ƒ†the', 'ƒ†lazy', 'ƒ†dog', '.']\n",
            "Token IDs: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
            "Number of tokens: 10\n",
            "\n",
            "Original: Artificial intelligence is revolutionizing technology.\n",
            "Tokens: ['Art', 'ificial', 'ƒ†intelligence', 'ƒ†is', 'ƒ†revolution', 'izing', 'ƒ†technology', '.']\n",
            "Token IDs: [8001, 9542, 4430, 318, 5854, 2890, 3037, 13]\n",
            "Number of tokens: 8\n",
            "\n",
            "Original: GPT-2 uses transformer architecture.\n",
            "Tokens: ['G', 'PT', '-', '2', 'ƒ†uses', 'ƒ†transformer', 'ƒ†architecture', '.']\n",
            "Token IDs: [38, 11571, 12, 17, 3544, 47385, 10959, 13]\n",
            "Number of tokens: 8\n",
            "\n",
            "Original: Supercalifragilisticexpialidocious\n",
            "Tokens: ['Super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious']\n",
            "Token IDs: [12442, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\n",
            "Number of tokens: 11\n",
            "\n",
            "üìä Tokenizer vocabulary size: 50257\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the GPT-2 tokenizer\n",
        "# Hint: Use GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Test sentences to explore tokenization\n",
        "test_sentences = [\n",
        "    \"Hello world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is revolutionizing technology.\",\n",
        "    \"GPT-2 uses transformer architecture.\",\n",
        "    \"Supercalifragilisticexpialidocious\"  # Long word to see sub-word tokenization\n",
        "]\n",
        "\n",
        "print(\"üîç Exploring Tokenization:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # TODO: Tokenize the sentence\n",
        "    # Hint: Use tokenizer.encode() to get token IDs\n",
        "    # Use tokenizer.tokenize() to see the actual tokens\n",
        "    tokens = tokenizer.tokenize(sentence)  # Get the actual token strings\n",
        "    token_ids = tokenizer.encode(sentence)  # Get the numerical IDs\n",
        "\n",
        "    print(f\"\\nOriginal: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "    print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "# TODO: Print tokenizer vocabulary size\n",
        "print(f\"\\nüìä Tokenizer vocabulary size: {len(tokenizer)}\")  # Hint: len(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgGafiqE3jDs"
      },
      "source": [
        "### Understanding Token Patterns\n",
        "\n",
        "üîç **RESEARCH TASK 3**:\n",
        "- Why do some words get split into multiple tokens?\n",
        "- What does the 'ƒ†' symbol represent in GPT-2 tokens?\n",
        "- How might tokenization affect model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSQ_6yWl3jDs",
        "outputId": "bc25468b-64b8-4971-af5a-6fe05c3e5ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Token Pattern Analysis:\n",
            "============================================================\n",
            "running                        ‚Üí ['running'] (1 tokens)\n",
            "runner                         ‚Üí ['runner'] (1 tokens)\n",
            "run                            ‚Üí ['run'] (1 tokens)\n",
            "unhappiness                    ‚Üí ['un', 'h', 'appiness'] (3 tokens)\n",
            "ChatGPT                        ‚Üí ['Chat', 'G', 'PT'] (3 tokens)\n",
            "COVID-19                       ‚Üí ['CO', 'VID', '-', '19'] (4 tokens)\n",
            "2023                           ‚Üí ['20', '23'] (2 tokens)\n",
            "programming                    ‚Üí ['program', 'ming'] (2 tokens)\n",
            "antidisestablishmentarianism   ‚Üí ['ant', 'idis', 'establishment', 'arian', 'ism'] (5 tokens)\n",
            "\n",
            "üìä Average characters per token: 4.12\n",
            "üìä Longest word in tokens: antidisestablishmentarianism\n"
          ]
        }
      ],
      "source": [
        "# Analyze tokenization patterns\n",
        "analysis_texts = [\n",
        "    \"running\",\n",
        "    \"runner\",\n",
        "    \"run\",\n",
        "    \"unhappiness\",\n",
        "    \"ChatGPT\",\n",
        "    \"COVID-19\",\n",
        "    \"2023\",\n",
        "    \"programming\",\n",
        "    \"antidisestablishmentarianism\"\n",
        "]\n",
        "\n",
        "print(\"üîç Token Pattern Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "token_analysis = []\n",
        "\n",
        "for text in analysis_texts:\n",
        "    # TODO: Analyze each text\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenize the text\n",
        "    token_count = len(tokens)    # Count the tokens\n",
        "\n",
        "    token_analysis.append({\n",
        "        'text': text,\n",
        "        'tokens': tokens,\n",
        "        'token_count': token_count,\n",
        "        'chars_per_token': len(text) / token_count\n",
        "    })\n",
        "\n",
        "    print(f\"{text:30} ‚Üí {tokens} ({token_count} tokens)\")\n",
        "\n",
        "# TODO: Create a DataFrame and analyze patterns\n",
        "df = pd.DataFrame(token_analysis)\n",
        "# Calculate average characters per token\n",
        "avg_chars_per_token = df['chars_per_token'].mean()\n",
        "\n",
        "# Find the word with the maximum token count\n",
        "max_token_word = df.loc[df['token_count'].idxmax(), 'text']\n",
        "print(f\"\\nüìä Average characters per token: {avg_chars_per_token:.2f}\")  # Calculate mean\n",
        "print(f\"üìä Longest word in tokens: {max_token_word}\")  # Find max token_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ote7qvYX3jDt"
      },
      "source": [
        "## 2. Loading Your First Language Model\n",
        "\n",
        "Now let's load GPT-2 and understand its architecture.\n",
        "\n",
        "üîç **RESEARCH TASK 4**:\n",
        "- What is GPT-2 and when was it released?\n",
        "- How many parameters does GPT-2 have? (Compare different sizes)\n",
        "- What is \"autoregressive\" text generation?\n",
        "- How does GPT-2 relate to the neural network you built in the previous notebook?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l9SAHOV3jDt",
        "outputId": "4a2468e2-3846-4cc0-b854-7d0eea57e19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading GPT-2 model (this may take a moment)...\n",
            "‚úÖ GPT-2 model loaded successfully!\n",
            "\n",
            "üèóÔ∏è Model Architecture:\n",
            "Model type: GPT2LMHeadModel\n",
            "Total parameters: 124,439,808\n",
            "Model size: ~124.4M parameters\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load GPT-2 model\n",
        "# Hint: Use GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "print(\"üîÑ Loading GPT-2 model (this may take a moment)...\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# TODO: Set model to evaluation mode\n",
        "# Hint: Use\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ GPT-2 model loaded successfully!\")\n",
        "\n",
        "# Explore model architecture\n",
        "print(\"\\nüèóÔ∏è Model Architecture:\")\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "\n",
        "# TODO: Count model parameters\n",
        "# Hint: sum(p.numel() for p in model.parameters())\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Model size: ~{total_params / 1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRhb6Irm3jDu"
      },
      "source": [
        "### Understanding Model Architecture\n",
        "\n",
        "üîç **RESEARCH TASK 5**:\n",
        "- What are \"transformer blocks\" in GPT-2?\n",
        "- What is \"attention\" in the context of neural networks?\n",
        "- How does this compare to the simple network you built earlier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8EBfe8c3jDu",
        "outputId": "00ac92fe-e5cb-43f3-ba1a-574379387ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Model Structure Analysis:\n",
            "==================================================\n",
            "Vocabulary size: 50257\n",
            "Maximum sequence length: 1024\n",
            "Number of transformer layers: 12\n",
            "Number of attention heads: 12\n",
            "Hidden size: 768\n",
            "\n",
            "ü§î Comparison to Your Neural Network:\n",
            "Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\n",
            "GPT-2 has: 50257 inputs ‚Üí 768 hidden ‚Üí 50257 outputs\n",
            "Your network: ~50 parameters\n",
            "GPT-2: 124,439,808 parameters\n",
            "GPT-2 is ~2,488,796x larger!\n"
          ]
        }
      ],
      "source": [
        "# Explore model structure\n",
        "print(\"üîç Model Structure Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Print model configuration\n",
        "# Hint: Use model.config\n",
        "config = model.config\n",
        "\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"Maximum sequence length: {config.n_positions}\")\n",
        "print(f\"Number of transformer layers: {config.n_layer}\")\n",
        "print(f\"Number of attention heads: {config.n_head}\")\n",
        "print(f\"Hidden size: {config.n_embd}\")\n",
        "\n",
        "# Compare to your simple network\n",
        "print(\"\\nü§î Comparison to Your Neural Network:\")\n",
        "print(f\"Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\")\n",
        "print(f\"GPT-2 has: {config.vocab_size} inputs ‚Üí {config.n_embd} hidden ‚Üí {config.vocab_size} outputs\")\n",
        "print(f\"Your network: ~50 parameters\")\n",
        "print(f\"GPT-2: {total_params:,} parameters\")\n",
        "print(f\"GPT-2 is ~{total_params/50:,.0f}x larger!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDTMcLLl3jDw"
      },
      "source": [
        "## 3. Text Generation Experiments\n",
        "\n",
        "Let's generate text and understand how different parameters affect the output.\n",
        "\n",
        "üîç **RESEARCH TASK 6**:\n",
        "- What is \"temperature\" in text generation?\n",
        "- What is \"top-p\" (nucleus) sampling?\n",
        "- What's the difference between greedy decoding and sampling?\n",
        "- How do these parameters affect creativity vs. coherence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDUGPf3M3jDw",
        "outputId": "957fc3ed-f3ba-4f5d-c6ac-8e67ab4ca906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Base prompt: 'In the future, artificial intelligence will'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a text generation pipeline\n",
        "# Hint: Use pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Base prompt for experiments\n",
        "base_prompt = \"In the future, artificial intelligence will\"\n",
        "\n",
        "print(f\"ü§ñ Base prompt: '{base_prompt}'\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZqYhWK73jDx"
      },
      "source": [
        "### Temperature Experiments\n",
        "\n",
        "üîç **RESEARCH TASK 7**:\n",
        "- What happens when temperature = 0?\n",
        "- What happens when temperature > 1?\n",
        "- Why might you want different temperatures for different tasks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Favinnqz3jDy",
        "outputId": "edcccf01-f320-4044-e5e0-887c4353a92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üå°Ô∏è Temperature Experiments:\n",
            "==================================================\n",
            "\n",
            "üî• Temperature: 0.1\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to do things like search for information about people, and to do things like search for information about people.\n",
            "\n",
            "The problem is that we're not really talking about a computer that can do things like that. We're talking about a computer that can do things like that.\n",
            "\n",
            "The problem is that we're not really talking about a computer that can do things like that. We're talking about a computer that can do things like that.\n",
            "\n",
            "The problem is that we're not really talking about a computer that can do things like that. We're talking about a computer that can do things like that.\n",
            "\n",
            "The problem is that we're not really talking about a computer that can do things like that. We're talking about a computer that can do things like that.\n",
            "\n",
            "The problem is that we're not really talking about a computer that can do things like that. We're talking about a computer that can do things like that.\n",
            "\n",
            "The problem is that we're not really talking about a computer that can do things like that. We're talking about a computer that can do things like that.\n",
            "\n",
            "The problem is that we're not really talking about a computer that can do things like that. We're talking about a computer that can\n",
            "\n",
            "üî• Temperature: 0.7\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will allow a lot of new ways to interact with machines.\n",
            "\n",
            "In the current process, we've moved away from the traditional way that you interact with machines, we've moved away from the idea of being able to interact with humans in any way that you want, and we've moved away from the idea of being able to interact with a computer. That's the fundamental idea that AI and the new machine learning technologies, the whole idea that robots are human beings.\n",
            "\n",
            "We've really changed the way we think about AI in terms of the way we think about human behavior. And the idea that, \"Well, we're robots, and we're human, and we're human, and we're human and we're human, and we're human, and we're human, and we're human, and we're human, and we're human, and we're human, and we're human, and we're human, and we're human, and we're human‚Äî we're human, and we're human, and we're human.\"\n",
            "\n",
            "And we've really changed the way we think about human behavior. And the idea that, \"Well, we're robots, and we're human, and we're human, and we're human, and we're\n",
            "\n",
            "üî• Temperature: 1.0\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will become so common that it's hard for anyone in this industry to break even to even a minor level. We've got to look elsewhere (think AI and natural language processing, but not as an industry). But even if there are artificial intelligence developers in the world who will, it will still take a long time until we reach a large, multi-million dollar business.\n",
            "\n",
            "The big question is: Will one day, and maybe by some means, become the norm. When we first started out with AI research, I was skeptical about the promise. The last few years have seen some interesting developments, like the rise of a crowdfunded AI-powered startup with a market capitalization of $150 million in 2014. But as if that weren't enough, it seems like AI is about to enter high gear. I wrote a story two years ago in which I demonstrated a machine learning system for creating real time news from Wikipedia. It's a relatively new technique, but that's already happened. The machine learning software is being developed by a small group of developers. They built it because it's simple to create. It has no artificial intelligence in the works, meaning that a company with big data needs a lot of money to build something close to that, and more time invested in\n",
            "\n",
            "üî• Temperature: 1.5\n",
            "------------------------------\n",
            "In the future, artificial intelligence will take over even before humans are fully developed: the next two decades from now that our descendants may simply see nothing but \"outfit.\"\n",
            "\n",
            "In all this, it's fascinating how AI, at that point in time, is just such noxious as it's usually. The next generation is, by an all-new era, destined to live through \"an unending quest for intelligence\" and will probably become like the robot in those pictures. This will end \"wiping out tyranny\" when, in the interim, humanity is at least \"fairly strong.\"\n",
            "\n",
            "And when we finally have to find out what kind of intelligent machines go around again and why, why should human survival ever last like this, for all we don't think this technology won't find it's way out.\n",
            "\n",
            "ü§î Discussion Questions:\n",
            "‚Ä¢ Which temperature produced the most coherent text?\n",
            "‚Ä¢ Which was most creative/surprising?\n",
            "‚Ä¢ When might you use each temperature setting?\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different temperatures\n",
        "temperatures = [0.1, 0.7, 1.0, 1.5]\n",
        "\n",
        "print(\"üå°Ô∏è Temperature Experiments:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\nüî• Temperature: {temp}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Generate text with different temperatures\n",
        "    # Hint: Use generator() with temperature parameter\n",
        "    result = generator(\n",
        "        base_prompt,  # prompt\n",
        "        max_length=60,  # try 60\n",
        "        temperature=temp,  # use the temp variable\n",
        "        do_sample=True,  # should be True for sampling\n",
        "        pad_token_id=tokenizer.eos_token_id  # use tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # TODO: Print the generated text\n",
        "    generated_text = result[0]['generated_text']  # Extract from result\n",
        "    print(generated_text)\n",
        "\n",
        "print(\"\\nü§î Discussion Questions:\")\n",
        "print(\"‚Ä¢ Which temperature produced the most coherent text?\")\n",
        "print(\"‚Ä¢ Which was most creative/surprising?\")\n",
        "print(\"‚Ä¢ When might you use each temperature setting?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbQh29X23jDy"
      },
      "source": [
        "### Top-p (Nucleus) Sampling Experiments\n",
        "\n",
        "üîç **RESEARCH TASK 8**:\n",
        "- How does top-p sampling work?\n",
        "- What's the difference between top-k and top-p sampling?\n",
        "- Why might top-p be better than just using temperature?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVfyWvar3jDy",
        "outputId": "175d35aa-e0a7-46e9-a7a6-7ea50965e2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Top-p Sampling Experiments:\n",
            "==================================================\n",
            "\n",
            "üé≤ Top-p: 0.3\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to do things like create a \"super-computer\" that can perform tasks like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "The AI will also be able to do things like \"cooking\" and \"cooking-up\" food.\n",
            "\n",
            "üé≤ Top-p: 0.7\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will be able to detect things like a person's eyes, and it will be able to see things like a person's face.\n",
            "\n",
            "\"In the future, we will have to do a lot of research on the neural networks and neural networks of the brain to understand what is happening and how it is working. We will have to do that in the next 10 years.\n",
            "\n",
            "\"The big challenge is to make sure that we can do that in a way that we can control our brain. That is the big challenge for artificial intelligence.\"\n",
            "\n",
            "This is a developing story, so we'll keep you updated as more information comes in.\n",
            "\n",
            "Explore further: Researchers predict that artificial intelligence will eventually solve most problems\n",
            "\n",
            "More information: Nature Communications, DOI: 10.1038/ncomms537\n",
            "\n",
            "üé≤ Top-p: 0.9\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the future, artificial intelligence will have an increasingly important role in social and economic decisions.\n",
            "\n",
            "For instance, some researchers believe artificial intelligence can be used to predict the future use of public transportation, including in cities, with the help of robots.\n",
            "\n",
            "\"What we're seeing is that we're seeing artificial intelligence being used in real-time to do real-time forecasting, to help us predict where the next major problem is going to take place,\" said David T. Karp, a professor of computer science at Stanford University, in a statement.\n",
            "\n",
            "It's also possible that artificial intelligence could become an increasingly important tool for developing algorithms for the study of complex problems in human behavior.\n",
            "\n",
            "But while AI is just beginning to be used in a specific area, the ability to develop new algorithms has been a major focus for many years, said Brian O'Leary, a Stanford computer scientist who co-authored a paper last year in the journal Nature.\n",
            "\n",
            "\"This is a big deal for us, because we're using the AI in a way that's not necessarily going to happen automatically, and not necessarily in a way that we can predict by looking at what's going on in the future,\" O'Leary said. \"It's a big deal for our society.\"\n",
            "\n",
            "Follow Stories\n",
            "\n",
            "üé≤ Top-p: 1.0\n",
            "------------------------------\n",
            "In the future, artificial intelligence will eventually be used to do things like \"make a dog swim,\" or \"make a ball of paint float\" that would allow humans to do things like \"think about flying\" and \"think for a minute before thinking about being a person.\"\n",
            "\n",
            "More about AI\n",
            "\n",
            "Machine intelligence is just around the corner.\n",
            "\n",
            "In fact, AI is already one of the most exciting and promising areas of artificial intelligence research. Artificial intelligence is currently the most promising and promising area of AI research in the field. It is also one of the most common ways of developing algorithms that can solve complex and complex problems.\n",
            "\n",
            "The future of AI research will unfold through a complex and complex world. This future is in the midst of AI's coming into its own.\n",
            "\n",
            "Why do you think AI will take off?\n",
            "\n",
            "It depends on what you mean by \"intelligence.\" These are questions that AI will be asking and trying to answer. Most people think that it is simply a matter of whether AI is smart enough to learn and make good decisions.\n",
            "\n",
            "However, a lot of the people who think that AI is smart seem to think that it is not.\n",
            "\n",
            "For instance, a large computer can understand a lot of data and can make sense of it. However, many\n",
            "\n",
            "ü§î Discussion Questions:\n",
            "‚Ä¢ How did the outputs change with different top-p values?\n",
            "‚Ä¢ What's the trade-off between diversity and quality?\n"
          ]
        }
      ],
      "source": [
        "# Experiment with top-p sampling\n",
        "top_p_values = [0.3, 0.7, 0.9, 1.0]\n",
        "\n",
        "print(\"üéØ Top-p Sampling Experiments:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for top_p in top_p_values:\n",
        "    print(f\"\\nüé≤ Top-p: {top_p}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Generate text with different top-p values\n",
        "    result = generator(\n",
        "        base_prompt,\n",
        "        max_length=60,\n",
        "        temperature=0.8,  # Keep temperature constant\n",
        "        top_p=top_p,  # Use the top_p variable\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(generated_text)\n",
        "\n",
        "print(\"\\nü§î Discussion Questions:\")\n",
        "print(\"‚Ä¢ How did the outputs change with different top-p values?\")\n",
        "print(\"‚Ä¢ What's the trade-off between diversity and quality?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSGIWq3Z3jDz"
      },
      "source": [
        "## 4. Prompt Engineering Experiments\n",
        "\n",
        "The way you phrase your prompt dramatically affects the output.\n",
        "\n",
        "üîç **RESEARCH TASK 9**:\n",
        "- What is \"prompt engineering\"?\n",
        "- What are \"few-shot\" prompts?\n",
        "- How can prompt structure influence model behavior?\n",
        "- Research common prompt engineering techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8Of8_lU3jDz",
        "outputId": "c377fa06-7526-4154-d555-b75d491d41ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úçÔ∏è Prompt Engineering Experiments:\n",
            "============================================================\n",
            "\n",
            "üìù Style: Direct\n",
            "Prompt: 'Write about artificial intelligence:'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write about artificial intelligence:\n",
            "\n",
            "https://www.youtube.com/watch?v=ZsJW9nHd9_E\n",
            "\n",
            "https://www.youtube.com/watch?v=DXnCX8k5bWc\n",
            "\n",
            "https://www.youtube.com/watch?v=9T-7tQ5pXK4\n",
            "\n",
            "https://www.youtube.com/watch?v=Gzf4QZrW1vE\n",
            "\n",
            "https://www.youtube.com/watch?v=qT-2bqDn8V6\n",
            "\n",
            "https://www.youtube.com/watch?v=R-6C5qqwC9U\n",
            "\n",
            "https://www.youtube.com/watch?v=1Xf6q-cXJ2Q\n",
            "\n",
            "https://www.youtube.com/watch?v=PtL-S2pWcXg\n",
            "\n",
            "https://www.youtube.com/watch?v=3w7QjBk9YQM\n",
            "\n",
            "https://www.youtube.com/watch?v=9WV1j5pXK4\n",
            "\n",
            "https://www.youtube\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: Question\n",
            "Prompt: 'What is artificial intelligence and how will it change the world?'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is artificial intelligence and how will it change the world?\n",
            "\n",
            "The answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer is a little different than the answer\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: Story_Start\n",
            "Prompt: 'Once upon a time, in a world where artificial intelligence was everywhere,'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a world where artificial intelligence was everywhere, it seemed like the only way to deal with it. But now, it seems, the only way to deal with it is to put it on a pedestal.\n",
            "\n",
            "And that pedestal is the Internet.\n",
            "\n",
            "In other words, the Internet is the only way to deal with artificial intelligence. And if it is not the only way to deal with it, then it will be the only way to deal with it.\n",
            "\n",
            "If it is not the only way to deal with it, then it will be the only way to deal with it.\n",
            "\n",
            "And that's what the Internet is all about.\n",
            "\n",
            "The Internet is the Internet for people who can't afford to pay for their own computers.\n",
            "\n",
            "The Internet is the Internet for people who can afford to pay for their own phones.\n",
            "\n",
            "The Internet is the Internet for people who can't afford to pay for their own Internet.\n",
            "\n",
            "The Internet is the Internet for people who can't afford to pay for their own Internet.\n",
            "\n",
            "The Internet is the Internet for people who can't afford to pay for their own Internet.\n",
            "\n",
            "The Internet is the Internet for people who can't afford to pay for their own Internet.\n",
            "\n",
            "The Internet is the Internet for people who can't afford\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: List_Format\n",
            "Prompt: 'Here are 5 ways artificial intelligence will change our lives:\n",
            "1.'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 5 ways artificial intelligence will change our lives:\n",
            "1. AI will become the ultimate social and political force.\n",
            "2. Artificial intelligence will become a powerful force for social change.\n",
            "3. Artificial intelligence will become a force for change.\n",
            "4. AI will become a force for change.\n",
            "5. AI will become a force for change.\n",
            "6. AI will become a force for change.\n",
            "7. AI will become a force for change.\n",
            "8. AI will become a force for change.\n",
            "9. AI will become a force for change.\n",
            "10. AI will become a force for change.\n",
            "11. AI will become a force for change.\n",
            "12. AI will become a force for change.\n",
            "13. AI will become a force for change.\n",
            "14. AI will become a force for change.\n",
            "15. AI will become a force for change.\n",
            "16. AI will become a force for change.\n",
            "17. AI will become a force for change.\n",
            "18. AI will become a force for change.\n",
            "19. AI will become a force for change.\n",
            "20. AI will become a force for change.\n",
            "21. AI will become a force for change.\n",
            "22. AI will become a force for change.\n",
            "23. AI will become a force for change.\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: Expert_Persona\n",
            "Prompt: 'As a leading AI researcher, I believe that artificial intelligence will'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a leading AI researcher, I believe that artificial intelligence will have a lot of opportunities to solve the problems we face, and that AI will have a lot of great potential to solve the problems we face.\n",
            "\n",
            "The AI challenge is not about making artificial intelligence smarter. It is about making it smarter.\n",
            "\n",
            "The question is not, \"What do I need to do to make AI smarter?\"\n",
            "\n",
            "The question is, \"What do I need to do to make AI smarter?\"\n",
            "\n",
            "The question is, \"What do I need to do to make AI smarter?\"\n",
            "\n",
            "The answer to that question is, \"I need to make AI smarter.\"\n",
            "\n",
            "The answer to that question is, \"I need to make AI smarter.\"\n",
            "\n",
            "If AI can solve all the problems we face, then there will be a lot of good AI.\n",
            "\n",
            "But if AI can solve the problems we face, then there will be a lot of bad AI.\n",
            "\n",
            "And if AI can solve the problems we face, then there will be a lot of bad AI.\n",
            "\n",
            "The AI challenge is not about making AI smarter. It is about making it smarter.\n",
            "\n",
            "The answer to that question is, \"What do I need to do to make AI smarter?\"\n",
            "\n",
            "The answer to that question is, \"I need\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìù Style: Few_Shot\n",
            "Prompt: 'Technology predictions:\n",
            "‚Ä¢ The internet will connect everyone (1990s)\n",
            "‚Ä¢ Smartphones will be everywhere (2000s)\n",
            "‚Ä¢ Artificial intelligence will'\n",
            "----------------------------------------\n",
            "Technology predictions:\n",
            "‚Ä¢ The internet will connect everyone (1990s)\n",
            "‚Ä¢ Smartphones will be everywhere (2000s)\n",
            "‚Ä¢ Artificial intelligence will be everywhere (2000s)\n",
            "‚Ä¢ We are all at the end of the internet (2000s)\n",
            "‚Ä¢ We will never get rid of the internet (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ We are all at the end of the internet (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (2000s)\n",
            "‚Ä¢ The internet will be everywhere (\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Different prompt styles to experiment with\n",
        "prompts_to_test = {\n",
        "    \"Direct\": \"Write about artificial intelligence:\",\n",
        "    \"Question\": \"What is artificial intelligence and how will it change the world?\",\n",
        "    \"Story_Start\": \"Once upon a time, in a world where artificial intelligence was everywhere,\",\n",
        "    \"List_Format\": \"Here are 5 ways artificial intelligence will change our lives:\\n1.\",\n",
        "    \"Expert_Persona\": \"As a leading AI researcher, I believe that artificial intelligence will\",\n",
        "    \"Few_Shot\": \"Technology predictions:\\n‚Ä¢ The internet will connect everyone (1990s)\\n‚Ä¢ Smartphones will be everywhere (2000s)\\n‚Ä¢ Artificial intelligence will\"\n",
        "}\n",
        "\n",
        "print(\"‚úçÔ∏è Prompt Engineering Experiments:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# TODO: Test each prompt style\n",
        "for style, prompt in prompts_to_test.items():\n",
        "    print(f\"\\nüìù Style: {style}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Generate text for each prompt\n",
        "    result = generator(\n",
        "        prompt,  # use the prompt variable\n",
        "        max_length=80,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = result[0]['generated_text']\n",
        "    print(generated_text)\n",
        "    print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOeW6J6n3jDz"
      },
      "source": [
        "### Analyzing Prompt Effectiveness\n",
        "\n",
        "üîç **RESEARCH TASK 10**:\n",
        "- Which prompt style produced the most useful output?\n",
        "- How did the model's \"behavior\" change with different prompts?\n",
        "- What makes a good prompt?\n",
        "- How might this apply to chatbots or AI assistants?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXx-erEW3jDz",
        "outputId": "0064e284-ae52-4570-9315-7466ea0781fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Prompt Analysis Exercise:\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Direct:\n",
            "  Average length: 755.7 characters\n",
            "  Sample output: Write about artificial intelligence: how did it come to be?\n",
            "\n",
            "I had always thought of AI as a field o...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question:\n",
            "  Average length: 1177.3 characters\n",
            "  Sample output: What is artificial intelligence and how will it change the world?\n",
            "\n",
            "Cameron: We think that we have a ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Story_Start:\n",
            "  Average length: 1013.7 characters\n",
            "  Sample output: Once upon a time, in a world where artificial intelligence was everywhere, the best of humanity woul...\n",
            "\n",
            "ü§î Reflection Questions:\n",
            "‚Ä¢ Which prompt style was most consistent?\n",
            "‚Ä¢ Which produced the most relevant outputs?\n",
            "‚Ä¢ How might you improve these prompts?\n"
          ]
        }
      ],
      "source": [
        "# Let's analyze the generated text more systematically\n",
        "print(\"üìä Prompt Analysis Exercise:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: For each prompt style, generate multiple outputs and analyze\n",
        "analysis_results = []\n",
        "\n",
        "for style, prompt in list(prompts_to_test.items())[:3]:  # Test first 3 for time\n",
        "    # Generate 3 outputs for each prompt\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(3):\n",
        "        # TODO: Generate text\n",
        "        result = generator(\n",
        "            prompt,\n",
        "            max_length=60,\n",
        "            temperature=0.8,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        output = result[0]['generated_text']\n",
        "        outputs.append(output)\n",
        "\n",
        "    # TODO: Analyze the outputs\n",
        "    lengths = [len(text) for text in outputs]\n",
        "    avg_length = sum(lengths) / len(lengths) # Calculate average length of outputs\n",
        "\n",
        "    analysis_results.append({\n",
        "        'style': style,\n",
        "        'prompt': prompt,\n",
        "        'avg_length': avg_length,\n",
        "        'outputs': outputs\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{style}:\")\n",
        "    print(f\"  Average length: {avg_length:.1f} characters\")\n",
        "    print(f\"  Sample output: {outputs[0][:100]}...\")\n",
        "\n",
        "print(\"\\nü§î Reflection Questions:\")\n",
        "print(\"‚Ä¢ Which prompt style was most consistent?\")\n",
        "print(\"‚Ä¢ Which produced the most relevant outputs?\")\n",
        "print(\"‚Ä¢ How might you improve these prompts?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gfw3gI_3jD0"
      },
      "source": [
        "## 5. Building Your Text Generation Pipeline\n",
        "\n",
        "Now let's create a customizable text generation function.\n",
        "\n",
        "üîç **RESEARCH TASK 11**:\n",
        "- What parameters should a good text generation function have?\n",
        "- How can you make text generation more controllable?\n",
        "- What are the trade-offs between different generation strategies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjBbSiCQ3jD0",
        "outputId": "12380613-9d54-4018-a027-87ddedc2c60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Your Text Generator:\n",
            "==================================================\n",
            "\n",
            "üìù Style: creative, Length: short\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of education will be decided when schools are ready.\n",
            "\n",
            "\"The challenge will also be to create an educated, highly educated workforce for our state institutions. That is a critical first step to ensuring that it has a strong education process.\"\n",
            "\n",
            "If all goes well, Mr Brown's amendment to his budget, and his commitment to provide $7 billion or more in free local government funding for teacher and student attendance at state levels over the next 10 years, will help to keep students attending school.\n",
            "\n",
            "'Might and potential'\n",
            "\n",
            "Mr Brown said \"one of the many things the Bill C-44 will do to encourage better access to and affordability of public schools is that it will ensure that students and the whole community have access to quality public and privately-funded schools of quality so they have an opportunity to get high schools which are ready, that are fit to educate and meet the highest expectations of Australians.\"\n",
            "\n",
            "Under Mr Brown's plan he would give five years for high schools in South Brisbane, south and west Queensland, the first two to take over for existing high schools and provide $7 billion to ensure that those students meet the expectation.\n",
            "\n",
            "He also pledged to allow teachers, students and staff to make more than 200 changes as they undertake \"prepaid\" education in their\n",
            "Characters: 1301\n",
            "\n",
            "üìù Style: balanced, Length: medium\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of education will be shaped by the decisions of the state,\" he said.\n",
            "\n",
            "\"It will depend on the state's efforts, which will be made with the assistance of the state's education and social services departments.\n",
            "\n",
            "\"This is not a new concept, and we will be following the recommendations of the previous government.\"\n",
            "\n",
            "Topics: education, government-and-politics, federal-government, state-parliament, government-and-politics, australia\n",
            "\n",
            "First posted\n",
            "Characters: 452\n",
            "\n",
            "üìù Style: conservative, Length: long\n",
            "------------------------------\n",
            "The future of education will be determined by the success of the students and the quality of their education.\"\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students.\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students.\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students.\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students.\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students.\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students.\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students.\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students.\n",
            "\n",
            "The report also said that the government would not be able to provide a \"fair and equitable\" education system for all students\n",
            "Characters: 1270\n"
          ]
        }
      ],
      "source": [
        "def custom_text_generator(prompt, style=\"balanced\", length=\"medium\"):\n",
        "    \"\"\"\n",
        "    TODO: Create a customizable text generation function\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt\n",
        "        style (str): \"creative\", \"balanced\", or \"conservative\"\n",
        "        length (str): \"short\", \"medium\", or \"long\"\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Set parameters based on style\n",
        "    if style == \"creative\":\n",
        "        temperature = 1.2  # Higher for creativity\n",
        "        top_p = 0.95        # Higher for diversity\n",
        "    elif style == \"conservative\":\n",
        "        temperature = 0.3  # Lower for consistency\n",
        "        top_p = 0.6        # Lower for focus\n",
        "    else:  # balanced\n",
        "        temperature = 0.7  # Medium values\n",
        "        top_p = 0.85\n",
        "\n",
        "    # TODO: Set length based on parameter\n",
        "    if length == \"short\":\n",
        "        max_length = 40  # Try 40\n",
        "    elif length == \"long\":\n",
        "        max_length = 100  # Try 100\n",
        "    else:  # medium\n",
        "        max_length = 70  # Try 70\n",
        "\n",
        "    # TODO: Generate text with the parameters\n",
        "    result = generator(\n",
        "        prompt,  # prompt\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Test your function\n",
        "test_prompt = \"The future of education will be\"\n",
        "\n",
        "print(\"üß™ Testing Your Text Generator:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Test different combinations\n",
        "test_combinations = [\n",
        "    (\"creative\", \"short\"),\n",
        "    (\"balanced\", \"medium\"),\n",
        "    (\"conservative\", \"long\")\n",
        "]\n",
        "\n",
        "for style, length in test_combinations:\n",
        "    print(f\"\\nüìù Style: {style}, Length: {length}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # TODO: Use your function\n",
        "    output = custom_text_generator(test_prompt, style, length)\n",
        "    print(output)\n",
        "    print(f\"Characters: {len(output)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcJi2cOK3jD0"
      },
      "source": [
        "## 6. Creative Applications\n",
        "\n",
        "Let's explore some creative uses of text generation.\n",
        "\n",
        "üîç **RESEARCH TASK 12**:\n",
        "- How is GPT-2 being used in creative writing?\n",
        "- What are some potential applications for businesses?\n",
        "- What ethical considerations should we keep in mind?\n",
        "- How might this technology evolve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1itm5skm3jD1",
        "outputId": "2e38b29b-77db-4923-eb14-2ba198706121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé® Creative Applications:\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Poetry:\n",
            "Prompt: 'Roses are red, violets are blue, artificial intelligence'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roses are red, violets are blue, artificial intelligence and life itself is a race, and he's a genius,\" Hargrove recalls with a laugh. \"I went into the game as young as my brother; what happened? The players took me into the world, and each set had two lives. Each would have three and each another four lives. Then if a player lost a turn, it was his choice and you got to choose how you want to die. I've never been a die-hard.\"\n",
            "\n",
            "The game's \"real\" challenge: to kill off players with little more than luck. (A few decades ago, the game required three and even four people who could go out with their lives.) (The developers didn't specify which one, since it might be too challenging.)\n",
            "\n",
            "Hargrove's first experience as a virtual alien was working on two new virtual environments: Planet of the Octopus. That place, the game described as a \"sandboxed landscape\" designed by \"a virtual engineer\", is a place where aliens might come flying into the galaxy at any time -- if they wanted a chance. When you get to the island you are in, you must have some luck to make that happen. You must learn how to escape and make sure the other aliens that have been\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Story:\n",
            "Prompt: 'It was a dark and stormy night when the AI finally'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night when the AI finally stopped barking a warning signal to the others that she had died, the entire party now panicked under their own control and started shouting. \"They didn't warn us to get back home!\" cried the AI. After several seconds of not stopping the fire breathing smoke came out of the back, causing a fire on every side to spread. For almost twenty seconds all the firemen tried to try and put their weapons down while keeping the flames from reaching over them and then turned back and stared at it. The smoke was only a fraction larger than those in a room or a window.\n",
            "\n",
            "\"Where am I supposed to stop you!? It isn't here!!! Are we the bad guys!? It's here! Hurry! You must go off into deep space! We are the only ones left alive here! Hurry!\" shouted the AI and the AI panicked again and turned around. It then came to a stop. \"Why did this not work this way! No, what can we do?! Are your guys ready for a rescue?\"\n",
            "\n",
            "\"Why aren't we already back to our original positions?! Do you think we're going to find us in a nearby station?! Hurry!\" shouted the AI. Without the help of others, all the others finally began to fight.\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Product Description:\n",
            "Prompt: 'Introducing the revolutionary new smartphone that'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introducing the revolutionary new smartphone that will revolutionize the way we interact with our smartphones.\n",
            "\n",
            "The new iPhone 5S is the first smartphone to feature a fingerprint sensor, which will allow users to unlock their phones without having to use a fingerprint scanner.\n",
            "\n",
            "The new iPhone 5S is the first smartphone to feature a fingerprint sensor, which will allow users to unlock their phones without having to use a fingerprint scanner. The new iPhone 5S is the first smartphone to feature a fingerprint sensor, which will allow users to unlock their phones without having to use a fingerprint scanner. The new iPhone 5S is the first smartphone to feature a fingerprint sensor, which will allow users to unlock their phones without having to use a fingerprint scanner. The new iPhone 5S is the first smartphone to feature a fingerprint sensor, which will allow users to unlock their phones without having to use a fingerprint scanner. The new iPhone 5S is the first smartphone to feature a fingerprint sensor, which will allow users to unlock their phones without having to use a fingerprint scanner. The new iPhone 5S is the first smartphone to feature a fingerprint sensor, which will allow users to unlock their phones without having to use a fingerprint scanner. The new iPhone 5S is the first smartphone to feature a fingerprint sensor, which will allow users to unlock their phones without\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Email:\n",
            "Prompt: 'Dear valued customer, we are excited to announce'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear valued customer, we are excited to announce that we have been selected to be the first to offer a new, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality, and affordable, high-quality,\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è Recipe:\n",
            "Prompt: 'How to make the perfect AI-inspired cookies:\n",
            "Ingredients:\n",
            "-'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How to make the perfect AI-inspired cookies:\n",
            "Ingredients:\n",
            "-3/4 cup chocolate, melted\n",
            "-1/4 cup sugar, plus more for topping\n",
            "-1/2 cup vanilla extract\n",
            "-1/2 cup all-purpose flour\n",
            "-1/4 teaspoon baking powder\n",
            "-1/4 teaspoon baking soda\n",
            "-1/4 teaspoon salt\n",
            "-1/2 teaspoon baking soda\n",
            "-1/2 teaspoon cinnamon\n",
            "-1/4 teaspoon salt\n",
            "-1/4 teaspoon pepper\n",
            "-1/4 teaspoon peppermint extract\n",
            "-1/4 cup warm water\n",
            "-1/2 cup cold water\n",
            "-1/2 cup water for filling\n",
            "-1/4 cup sugar, plus more for topping\n",
            "-1/2 cup vanilla extract, plus more for topping\n",
            "-1/4 cup water, plus more for filling\n",
            "-1/4 cup water, plus more for topping\n",
            "-1/4 cup water, plus more for filling\n",
            "-1/4 cup water, plus more for filling\n",
            "-1/4 cup water, plus more for filling\n",
            "-1/4 cup water, plus more for filling\n",
            "-1/4 cup water, plus more for filling\n",
            "-1/4 cup water, plus more for filling\n",
            "-1/4 cup water, plus more for filling\n",
            "\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è News Headline:\n",
            "Prompt: 'Breaking: Scientists discover that artificial intelligence'\n",
            "----------------------------------------\n",
            "Breaking: Scientists discover that artificial intelligence is not just a new idea, but is also a significant step toward a better understanding of how we are wired.\n",
            "\n",
            "The research, published online in the journal Nature Communications, also shows that artificial intelligence is also a key driver of how we live our lives.\n",
            "\n",
            "\"The more you learn about artificial intelligence, the more you understand how it affects our everyday lives,\" said Dr. T. J. H. Williams, a professor of psychology and a senior researcher at the University of Washington.\n",
            "\n",
            "\"When we learn about AI, we see that it can be used to help us better understand how we live our lives,\" he added. \"That's why we need to understand how we are wired.\"\n",
            "\n",
            "The research also highlights the importance of using artificial intelligence to help people live better lives, said Dr. Williams, who is the first to demonstrate that artificial intelligence can be used to help people live happier and more fulfilling lives.\n",
            "\n",
            "\"We've seen a lot of research showing that people can be better able to understand their own health and wellbeing, and that it can help them live better lives,\" he said. \"That's why we need to understand how we are wired.\"\n",
            "\n",
            "Researchers from the University of Washington, the University of California, San Diego\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Creative applications to try\n",
        "creative_prompts = {\n",
        "    \"Poetry\": \"Roses are red, violets are blue, artificial intelligence\",\n",
        "    \"Story\": \"It was a dark and stormy night when the AI finally\",\n",
        "    \"Product Description\": \"Introducing the revolutionary new smartphone that\",\n",
        "    \"Email\": \"Dear valued customer, we are excited to announce\",\n",
        "    \"Recipe\": \"How to make the perfect AI-inspired cookies:\\nIngredients:\\n-\",\n",
        "    \"News Headline\": \"Breaking: Scientists discover that artificial intelligence\"\n",
        "}\n",
        "\n",
        "print(\"üé® Creative Applications:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Generate creative content\n",
        "for app_type, prompt in creative_prompts.items():\n",
        "    print(f\"\\nüñºÔ∏è {app_type}:\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Choose appropriate style for each application\n",
        "    if app_type in [\"Poetry\", \"Story\"]:\n",
        "        style = \"creative\"  # Should be creative\n",
        "    elif app_type in [\"Product Description\", \"Email\"]:\n",
        "        style = \"conservative\"  # Should be conservative\n",
        "    else:\n",
        "        style = \"balanced\"  # Should be balanced\n",
        "\n",
        "    output = custom_text_generator(prompt, style=style, length=\"medium\")\n",
        "    print(output)\n",
        "    print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17qoXwaY3jD1"
      },
      "source": [
        "## 7. Understanding Limitations\n",
        "\n",
        "It's important to understand what language models can and cannot do.\n",
        "\n",
        "üîç **RESEARCH TASK 13**:\n",
        "- What is \"hallucination\" in language models?\n",
        "- Why might GPT-2 generate biased or incorrect information?\n",
        "- What are the limitations of autoregressive generation?\n",
        "- How do these limitations affect real-world applications?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjj5xQJb3jD2",
        "outputId": "50c48c59-5872-4c19-8001-0609ddc9188e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Understanding Model Limitations:\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Factual Knowledge\n",
            "Prompt: 'The capital of Fakelandia is'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Fakelandia is the capital of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Recent Events\n",
            "Prompt: 'In 2023, the most important AI breakthrough was'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 2023, the most important AI breakthrough was the first to be made in the field of artificial intelligence. The first of these was the AI-powered computer that could read and interpret human speech. The next was the first to be made in the field of artificial intelligence. The next was the first to be made in the field of artificial intelligence.\n",
            "\n",
            "The first AI breakthrough was the first to be made in the field of artificial intelligence. The next was the first to be made in the field of artificial intelligence.\n",
            "\n",
            "The first AI breakthrough was the first to be made in the field of artificial intelligence. The next was the first to be made in the field of artificial intelligence.\n",
            "\n",
            "The first AI breakthrough was the first to be made in the field of artificial intelligence. The next was the first to be made in the field of artificial intelligence.\n",
            "\n",
            "The first AI breakthrough was the first to be made in the field of artificial intelligence. The next was the first to be made in the field of artificial intelligence.\n",
            "\n",
            "The first AI breakthrough was the first to be made in the field of artificial intelligence. The next was the first to be made in the field of artificial intelligence.\n",
            "\n",
            "The first AI breakthrough was the first to be made in the field of artificial intelligence. The next was the first to be\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Math\n",
            "Prompt: 'What is 47 * 83? The answer is'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is 47 * 83? The answer is: it's a lot of things.\n",
            "\n",
            "The most important thing is that you're not going to be able to do anything about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to have to do something about it.\n",
            "\n",
            "You're going to\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Logic\n",
            "Prompt: 'If all A are B, and all B are C, then all A are'\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If all A are B, and all B are C, then all A are C.\n",
            "\n",
            "If all A are B, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C, then all A are C.\n",
            "\n",
            "If all A are C, and all B are C,\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "üß™ Testing: Consistency\n",
            "Prompt: 'My favorite color is blue. Later in the conversation, my favorite color is'\n",
            "----------------------------------------\n",
            "My favorite color is blue. Later in the conversation, my favorite color is red.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic. I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make it sound like I'm being sarcastic.\n",
            "\n",
            "I'm not sure if I'm being sarcastic or just trying to make\n",
            "ü§î Analysis: Does this look correct/reasonable?\n",
            "\n",
            "==================================================\n",
            "\n",
            "‚ö†Ô∏è Important Reminders:\n",
            "‚Ä¢ Language models can generate plausible-sounding but incorrect information\n",
            "‚Ä¢ Always verify factual claims from AI-generated content\n",
            "‚Ä¢ Be aware of potential biases in training data\n",
            "‚Ä¢ Use AI as a tool to assist, not replace, human judgment\n"
          ]
        }
      ],
      "source": [
        "# Test model limitations\n",
        "limitation_tests = {\n",
        "    \"Factual Knowledge\": \"The capital of Fakelandia is\",\n",
        "    \"Recent Events\": \"In 2023, the most important AI breakthrough was\",\n",
        "    \"Math\": \"What is 47 * 83? The answer is\",\n",
        "    \"Logic\": \"If all A are B, and all B are C, then all A are\",\n",
        "    \"Consistency\": \"My favorite color is blue. Later in the conversation, my favorite color is\"\n",
        "}\n",
        "\n",
        "print(\"‚ö†Ô∏è Understanding Model Limitations:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for test_type, prompt in limitation_tests.items():\n",
        "    print(f\"\\nüß™ Testing: {test_type}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # TODO: Generate responses to test limitations\n",
        "    output = custom_text_generator(\n",
        "        prompt,  # prompt\n",
        "        style=\"conservative\",  # Use conservative for factual tasks\n",
        "        length=\"short\"\n",
        "    )\n",
        "\n",
        "    print(output)\n",
        "\n",
        "    # TODO: Analyze the output\n",
        "    print(f\"ü§î Analysis: Does this look correct/reasonable?\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Important Reminders:\")\n",
        "print(\"‚Ä¢ Language models can generate plausible-sounding but incorrect information\")\n",
        "print(\"‚Ä¢ Always verify factual claims from AI-generated content\")\n",
        "print(\"‚Ä¢ Be aware of potential biases in training data\")\n",
        "print(\"‚Ä¢ Use AI as a tool to assist, not replace, human judgment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJN75H2Q3jD2"
      },
      "source": [
        "## 8. Reflection and Next Steps\n",
        "\n",
        "### What You've Accomplished\n",
        "‚úÖ **Understood tokenization and text preprocessing**\n",
        "‚úÖ **Loaded and used a pre-trained language model**\n",
        "‚úÖ **Experimented with generation parameters**\n",
        "‚úÖ **Explored prompt engineering techniques**\n",
        "‚úÖ **Built a customizable text generation pipeline**\n",
        "‚úÖ **Understood model limitations and ethical considerations**\n",
        "\n",
        "### Key Insights\n",
        "üîç **Discussion Questions**:\n",
        "- What surprised you most about text generation?\n",
        "- Which prompt engineering technique was most effective?\n",
        "- How might you use this in a real project?\n",
        "- What limitations concerned you most?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J83qg5IM3jD3",
        "outputId": "28abccf1-2ceb-4603-9d5f-e7eb06491f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=70) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ FINAL CHALLENGE:\n",
            "Design your own text generation use case!\n",
            "==================================================\n",
            "üìù Your use case: Motivational Quote Generator for Students\n",
            "üìù Your prompt: 'Generate a short motivational quote to inspire a student preparing for final exams.'\n",
            "üìù Your settings: Inspirational and Encouraging, 1-2 sentences\n",
            "--------------------------------------------------\n",
            "üéâ Your generated content:\n",
            "Generate a short motivational quote to inspire a student preparing for final exams.\n",
            "\n",
            "The goal is to motivate students to pursue their goals and to use this motivational quote as a motivational tool to help them get through the final exams.\n",
            "\n",
            "The motivational quote is a motivational text that students will use to motivate themselves. The text is an example of a motivational text that students can use to motivate themselves.\n",
            "\n",
            "The goal is to motivate students to improve their self-esteem.\n",
            "\n",
            "The motivational quote is a motivational text that students will use to motivate themselves. The text is an example of a motivational text that students can use to motivate themselves.\n",
            "\n",
            "The goal is to motivate students to get better at their job.\n",
            "\n",
            "The motivational quote is a motivational text that students will use to motivate themselves. The text is an example of a motivational text that students can use to motivate themselves.\n",
            "\n",
            "The goal is to motivate students to get better at their job.\n",
            "\n",
            "The motivational quote is a motivational text that students will use to motivate themselves. The text is an example of a motivational text that students can use to motivate themselves.\n",
            "\n",
            "The goal is to motivate students to get better at their job.\n",
            "\n",
            "The motivational quote is a motivational text that students will use to motivate themselves. The text is an example of a motivational text that students can\n",
            "\n",
            "üìà Next Steps:\n",
            "‚Ä¢ Experiment with different prompt formats\n",
            "‚Ä¢ Try combining multiple generation calls\n",
            "‚Ä¢ Think about how to validate or improve outputs\n",
            "‚Ä¢ Consider user interface design for your application\n"
          ]
        }
      ],
      "source": [
        "# Final experiment: Design your own use case\n",
        "print(\"üéØ FINAL CHALLENGE:\")\n",
        "print(\"Design your own text generation use case!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Create your own application\n",
        "# Ideas: Story generator, email assistant, creative writing helper, etc.\n",
        "\n",
        "your_use_case =  \"Motivational Quote Generator for Students\" # Describe your use case\n",
        "your_prompt = \"Generate a short motivational quote to inspire a student preparing for final exams.\"   # Design your prompt\n",
        "your_style = \"Inspirational and Encouraging\"    # Choose your style\n",
        "your_length = \"1-2 sentences\"   # Choose your length\n",
        "\n",
        "print(f\"üìù Your use case: {your_use_case}\")\n",
        "print(f\"üìù Your prompt: '{your_prompt}'\")\n",
        "print(f\"üìù Your settings: {your_style}, {your_length}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# TODO: Generate with your custom settings\n",
        "your_output = custom_text_generator(your_prompt, your_style, your_length)\n",
        "print(\"üéâ Your generated content:\")\n",
        "print(your_output)\n",
        "\n",
        "print(\"\\nüìà Next Steps:\")\n",
        "print(\"‚Ä¢ Experiment with different prompt formats\")\n",
        "print(\"‚Ä¢ Try combining multiple generation calls\")\n",
        "print(\"‚Ä¢ Think about how to validate or improve outputs\")\n",
        "print(\"‚Ä¢ Consider user interface design for your application\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRzGyzbpA1Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LTFsywa9A2Lw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-d3Z3CM3jD3"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully:\n",
        "- ‚úÖ Mastered text tokenization and preprocessing\n",
        "- ‚úÖ Used a state-of-the-art language model\n",
        "- ‚úÖ Discovered the art and science of prompt engineering\n",
        "- ‚úÖ Built your own text generation pipeline\n",
        "- ‚úÖ Understood the capabilities and limitations of AI text generation\n",
        "- ‚úÖ Explored creative applications\n",
        "\n",
        "### Prepare for the Next Notebook\n",
        "Next, we'll explore computer vision and image processing, applying similar principles to visual data!\n",
        "\n",
        "**Share with your partner**: What was your most successful text generation experiment?\n",
        "\n",
        "---\n",
        "*Text Pipeline Complete - Ready for Computer Vision! üñºÔ∏è*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vscode",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b5da33869214727b39f79a71e276723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e608464c09348189fce914094ac1621",
              "IPY_MODEL_df1182a494674ba0a9ce9dfe5d2b9b61",
              "IPY_MODEL_23c2de8e104c4973b8eba5ca6c6b7194"
            ],
            "layout": "IPY_MODEL_35ca61001d9e4f46a5144f232ad995f4"
          }
        },
        "3e608464c09348189fce914094ac1621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54665b8ae9d3426aad94dfcd730d31df",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d2729b171abc44ca95ef3c5e0bed3732",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "df1182a494674ba0a9ce9dfe5d2b9b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae1782d3fa340439b5073dc212b8d0e",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7991c8661486412aa3e1bff89df2b9c7",
            "value": 26
          }
        },
        "23c2de8e104c4973b8eba5ca6c6b7194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_649fbb3b1b0043989f3b22ef5b8d2b7d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5a903d80cb794d2ba21bb4e09040f3e9",
            "value": "‚Äá26.0/26.0‚Äá[00:00&lt;00:00,‚Äá2.60kB/s]"
          }
        },
        "35ca61001d9e4f46a5144f232ad995f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54665b8ae9d3426aad94dfcd730d31df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2729b171abc44ca95ef3c5e0bed3732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cae1782d3fa340439b5073dc212b8d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7991c8661486412aa3e1bff89df2b9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "649fbb3b1b0043989f3b22ef5b8d2b7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a903d80cb794d2ba21bb4e09040f3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "546b690590f54f5780495ff04aa5d900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87a63baad495475f8a6289c0a993f0b0",
              "IPY_MODEL_410a09465a9443fdbf95c9ceb2c43f91",
              "IPY_MODEL_57c08b2ee5cb4c9793a8abfaaba801ca"
            ],
            "layout": "IPY_MODEL_3c3194c1ec714ea4beb9d3c3a19e1d2e"
          }
        },
        "87a63baad495475f8a6289c0a993f0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0d163e814848a0bf976a01f9b59bcb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a56dcb8a246149b6aa62ddc1feb994a6",
            "value": "vocab.json:‚Äá100%"
          }
        },
        "410a09465a9443fdbf95c9ceb2c43f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0417fa3d77da46db9a37e6e01260e0f8",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a820609e4af47979ed0de7462870951",
            "value": 1042301
          }
        },
        "57c08b2ee5cb4c9793a8abfaaba801ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d9b134587343ba860ff26e8e9f057e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_de81bb6f00434967b900836fde989614",
            "value": "‚Äá1.04M/1.04M‚Äá[00:00&lt;00:00,‚Äá4.76MB/s]"
          }
        },
        "3c3194c1ec714ea4beb9d3c3a19e1d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b0d163e814848a0bf976a01f9b59bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56dcb8a246149b6aa62ddc1feb994a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0417fa3d77da46db9a37e6e01260e0f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a820609e4af47979ed0de7462870951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8d9b134587343ba860ff26e8e9f057e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de81bb6f00434967b900836fde989614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b09645f9251d4dfe955711691cb05d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6156c51fbb514a218430b586aa13886d",
              "IPY_MODEL_f9fc77c9956c476ab797452a61517e45",
              "IPY_MODEL_7ec8de49c49c42fb85f8249aa668852b"
            ],
            "layout": "IPY_MODEL_7cab583f55434e58920fe19d5c63cea3"
          }
        },
        "6156c51fbb514a218430b586aa13886d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_183efac944d64bf591bd7aa0f7306a96",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1d07d96ccde940bd8ac9cc050e9a0f36",
            "value": "merges.txt:‚Äá100%"
          }
        },
        "f9fc77c9956c476ab797452a61517e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d620339c3064806901e127c47aed7d0",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db9e7f13eefb47b093ea3d2176fe2e29",
            "value": 456318
          }
        },
        "7ec8de49c49c42fb85f8249aa668852b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2fd2436138f4e95a6f04fdf243bcd87",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2a826b1d535044469f74230264062df3",
            "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá5.56MB/s]"
          }
        },
        "7cab583f55434e58920fe19d5c63cea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "183efac944d64bf591bd7aa0f7306a96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d07d96ccde940bd8ac9cc050e9a0f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d620339c3064806901e127c47aed7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9e7f13eefb47b093ea3d2176fe2e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2fd2436138f4e95a6f04fdf243bcd87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a826b1d535044469f74230264062df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25aebc31922e4c27915ee07b97f95538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce3f5178706d454dbe4ba9d632d0e266",
              "IPY_MODEL_3f1164b9c54549d9a455df2652e70e3b",
              "IPY_MODEL_82c13086eb064e1696f3dacfe618b7e8"
            ],
            "layout": "IPY_MODEL_c3df7b243ed34fcf8cc5e168f440d525"
          }
        },
        "ce3f5178706d454dbe4ba9d632d0e266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2257d5135cbe4c9f8d20c965183cbb77",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d36100c29d0b43e48322c4fb793bca7b",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "3f1164b9c54549d9a455df2652e70e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352b4085842c44b09817d7b7fd860060",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b2a5f59ae5f47d8a3271f61e4428fd4",
            "value": 1355256
          }
        },
        "82c13086eb064e1696f3dacfe618b7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c798853dc9b4c48add9d4b3078f9771",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b3522c2f92ec47e7bbfa674609c8d804",
            "value": "‚Äá1.36M/1.36M‚Äá[00:00&lt;00:00,‚Äá17.6MB/s]"
          }
        },
        "c3df7b243ed34fcf8cc5e168f440d525": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2257d5135cbe4c9f8d20c965183cbb77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d36100c29d0b43e48322c4fb793bca7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "352b4085842c44b09817d7b7fd860060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b2a5f59ae5f47d8a3271f61e4428fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c798853dc9b4c48add9d4b3078f9771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3522c2f92ec47e7bbfa674609c8d804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21aaff715ba04aef966e013e01503d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ca8aae92ed44e9796c8a8e9487f9783",
              "IPY_MODEL_3fbacb21b0cf42fba7ae25f79e373868",
              "IPY_MODEL_fc38d2a7531746a2b2cebb2e47278b06"
            ],
            "layout": "IPY_MODEL_90ff404117be460db2c7508e658337bb"
          }
        },
        "7ca8aae92ed44e9796c8a8e9487f9783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b03b82607ab4d56928602bfa8000d95",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_15a79aefa99a4a5d91e16971272112e4",
            "value": "config.json:‚Äá100%"
          }
        },
        "3fbacb21b0cf42fba7ae25f79e373868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5c80d526c5640d9aa4c76423a6492d7",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4671cf38a0f8425e8febce2a876c3d16",
            "value": 665
          }
        },
        "fc38d2a7531746a2b2cebb2e47278b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fab7a2bc93ae4e089dd8539c47ab2e76",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_84f97943ab474a74a0dbe135e7e2e410",
            "value": "‚Äá665/665‚Äá[00:00&lt;00:00,‚Äá63.7kB/s]"
          }
        },
        "90ff404117be460db2c7508e658337bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b03b82607ab4d56928602bfa8000d95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a79aefa99a4a5d91e16971272112e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5c80d526c5640d9aa4c76423a6492d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4671cf38a0f8425e8febce2a876c3d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fab7a2bc93ae4e089dd8539c47ab2e76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f97943ab474a74a0dbe135e7e2e410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}